{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdImr9RJdbZRMNt0HVsRv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdopleAIOrg/Video-Transcribe/blob/main/video_transciber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uklE2Jt3gYid"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import subprocess\n",
        "from simple_diarizer.diarizer import Diarizer\n",
        "import streamlit as st\n",
        "import base64\n",
        "import tempfile\n",
        "\n",
        "\n",
        "class VideoTranscriber:\n",
        "\n",
        "    def create_download_link(self,val, filename, label):\n",
        "\n",
        "        '''Hack to have a stable download link in Streamlit'''\n",
        "\n",
        "        b64 = base64.b64encode(val)\n",
        "        return f'<a href=\"data:application/octet-stream;base64,{b64.decode()}\" download=\"{filename}\">{label}</a>'\n",
        "\n",
        "    def segment(self,nu_speakers,temp_file):\n",
        "\n",
        "        '''Segment the audio using simple_diarizer.\n",
        "        Defaults to the speechbrain ECAPA-TDNN embeddings.'''\n",
        "\n",
        "        diar = Diarizer(embed_model='ecapa',cluster_method='sc')\n",
        "        segments = diar.diarize(temp_file, num_speakers=nu_speakers)\n",
        "\n",
        "        sdf = pd.DataFrame(segments)\n",
        "\n",
        "        # reorganize so the first speaker is always speaker 1\n",
        "        speaker_s = sdf['label'].drop_duplicates().reset_index()['label']\n",
        "        speaker_d = dict((v,k+1) for k,v in speaker_s.items())\n",
        "\n",
        "        sdf['speaker'] = sdf['label'].replace(speaker_d)\n",
        "        return sdf\n",
        "\n",
        "    def monotize(self,uploaded,temp_file):\n",
        "\n",
        "        '''Convert the upload file to audio file.'''\n",
        "\n",
        "        cmd = f\"ffmpeg -y -i {uploaded} -acodec pcm_s16le -ar 16000 -ac 1 {temp_file}\"\n",
        "        subprocess.Popen(cmd, shell=True).wait()\n",
        "\n",
        "    def audio_to_df(self,uploaded,model_size,temp_file,task):\n",
        "\n",
        "        '''Turn the upload file in a segemented dataframe.'''\n",
        "\n",
        "        #monotize(uploaded)\n",
        "        model = whisper.load_model(model_size)\n",
        "        result = model.transcribe(temp_file,\n",
        "                                  without_timestamps=False,\n",
        "                                  task = task)\n",
        "        tdf = pd.DataFrame(result['segments'])\n",
        "        return tdf\n",
        "\n",
        "    def add_preface(self,row):\n",
        "\n",
        "        ''' Add speaker prefix to transcript during transcribe().'''\n",
        "\n",
        "        text = row['text'].replace('\\n','')\n",
        "        speaker = row['speaker']\n",
        "        return f'Speaker {speaker}: {text}'\n",
        "\n",
        "    def transcribe(self,uploaded, nu_speakers, temp_file, model_size, task):\n",
        "\n",
        "        # Convert file to mono\n",
        "        with st.spinner(text=\"Converting file...\"):\n",
        "            self.monotize('temp_audio',temp_file)\n",
        "\n",
        "        # Make audio available to play in UI\n",
        "        audio_file = open(temp_file, 'rb')\n",
        "        audio_bytes = audio_file.read()\n",
        "        st.audio(temp_file, format='audio/wav')\n",
        "\n",
        "        # trancibe file\n",
        "        with st.spinner(text=f\"Transcribing using {model_size} model...\"):\n",
        "            tdf = self.audio_to_df(uploaded, model_size, temp_file, task)\n",
        "        # segement file\n",
        "        with st.spinner(text=\"Segmenting...\"):\n",
        "            sdf = self.segment(nu_speakers,temp_file)\n",
        "\n",
        "        # Find the nearest transcript line to the start of each speaker\n",
        "        ns_list = sdf[['start','speaker']].to_dict(orient='records')\n",
        "        for row in ns_list:\n",
        "            input = row['start']\n",
        "            id = tdf.iloc[(tdf['start']-input).abs().argsort()[:1]]['id'].values[0]\n",
        "            tdf.loc[tdf['id'] ==id, 'speaker'] = row['speaker']\n",
        "        tdf['speaker'].fillna(method = 'ffill', inplace = True)\n",
        "        tdf['speaker'].fillna(method = 'bfill', inplace = True)\n",
        "        tdf['n1'] = tdf['speaker'] != tdf['speaker'].shift(1)\n",
        "        tdf['speach'] = tdf['n1'].cumsum()\n",
        "\n",
        "        # collaps the dataframe by speach turn.\n",
        "        binned_df = tdf.groupby(['speach', 'speaker'])['text'].apply('\\n'.join).reset_index()\n",
        "        binned_df['speaker'] = binned_df['speaker'].astype(int)\n",
        "        binned_df['output'] = binned_df.apply(self.add_preface, axis=1)\n",
        "\n",
        "        # Display the transcript and prepare for export\n",
        "        lines = []\n",
        "        for row in binned_df['output'].values:\n",
        "            st.write(row)\n",
        "            lines.append(row)\n",
        "        tdf['speaker'] = tdf['speaker'].astype(int)\n",
        "\n",
        "        tdf_cols = ['speaker','start','end','text']\n",
        "\n",
        "        #st.dataframe(tdf[tdf_cols])\n",
        "        return {'text':lines, 'df': tdf[tdf_cols]}\n",
        "\n",
        "    def video_submission(self):\n",
        "\n",
        "        st.title(\"Video To Transcription\")\n",
        "\n",
        "        form = st.form(key='my_form')\n",
        "        uploaded = form.file_uploader(\"Choose a file\")\n",
        "        nu_speakers = form.slider('Number of speakers in recording:', min_value=1, max_value=8, value=2, step=1)\n",
        "        models = form.selectbox(\n",
        "            'Which Whisper model?',\n",
        "            ('Tiny (fast)', 'Base (good)', 'Small (great but slow)', 'Medium (greater but slower)'), index=1)\n",
        "        translate = form.checkbox('Translate to English?')\n",
        "        submit = form.form_submit_button(\"Transcribe!\")\n",
        "\n",
        "        return submit, models, translate, nu_speakers, uploaded\n",
        "\n",
        "    def choose_model(self,submit, models, translate, nu_speakers, uploaded):\n",
        "\n",
        "        if submit:\n",
        "            if models == 'Tiny (fast)':\n",
        "                model_size = 'tiny'\n",
        "            elif models == 'Base (good)':\n",
        "                model_size ='base'\n",
        "            elif models == 'Small (great but slow)':\n",
        "                model_size = 'small'\n",
        "            elif models == 'Medium (greater but slower)':\n",
        "                model_size = 'medium'\n",
        "\n",
        "            if translate == True:\n",
        "                task = 'translate'\n",
        "            else:\n",
        "                task = 'transcribe'\n",
        "\n",
        "            #temporary file to store audio_file\n",
        "            tmp_dir = tempfile.TemporaryDirectory()\n",
        "            temp_file = tmp_dir.name + '/mono.wav'\n",
        "\n",
        "            bytes_data = uploaded.getvalue()\n",
        "            with open('temp_audio', 'wb') as outfile:\n",
        "                outfile.write(bytes_data)\n",
        "\n",
        "\n",
        "            # Transcribe/translate and segment\n",
        "            transcript = self.transcribe('temp_audio', nu_speakers, temp_file, model_size, task)\n",
        "\n",
        "            # Prepare text file for export.\n",
        "            text = '\\n'.join(transcript['text']).encode('utf-8')\n",
        "            download_url = self.create_download_link(text, 'transcript.txt', 'Download transcript as plain text.')\n",
        "            st.markdown(download_url, unsafe_allow_html=True)\n",
        "\n",
        "            # prepare CSV file for expport.\n",
        "            csv = transcript['df'].to_csv( float_format='%.2f', index=False).encode('utf-8')\n",
        "            download_url = self.create_download_link(csv, 'transcript.csv', 'Download transcript as CSV (with time codes)')\n",
        "            st.markdown(download_url, unsafe_allow_html=True)\n",
        "            tmp_dir.cleanup()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    transcriber = VideoTranscriber()\n",
        "    submit, models, translate, nu_speakers, uploaded = transcriber.video_submission()\n",
        "    transcriber.choose_model(submit, models, translate, nu_speakers, uploaded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo9PYDNygju_",
        "outputId": "a93d9aa9-77f5-4746-9288-5755f6874e3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "oooBCOnDrYWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}